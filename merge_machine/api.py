#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb  6 15:01:16 2017

@author: leo

TODO:
    - Safe file name / not unique per date
    
    - API: List of internal referentials
    - API: List of finished modules for given project / source
    - API: List of loaded sources
    
    - API: Fetch infered parameters
    - API: Fetch logs
    - API: Move implicit load out of API
    
    - API: Error codes / remove error
    
    - API: DOWNLOAD CONFIG and run pipeline
    
    - Use logging module
    
    - Change metadata to use_internal and ref_name to last used or smt. Data to
      use is specified on api call and not read from metadata (unless using last used)
    
    - Protect admin functions
    -
    - ADD LICENSE

    - Gray out next button until all fields are properly filled
    - Do not go to next page if an error occured
    - General error handling
    
    - transform_and_download button
    - Download config
    
    - DOWNLOAD full config
    
    - ADD Metadata to file upload (file name, date of validity)
    - User account
    - Auto train 
    
    - ABSOLUTELY: CHANGE to user context rather than _app_ctx_stack . handle memory issues
    - Allocate memory by user/ by IP?
    
    - TODO: look why I can get 90% or 30% match on same file
    - Study impact of training set size on match rate
    - POSTGRES all this ish
    
    - Choose btw add/select/upload and read/load/get
    
    - Catch exceptions. Never redirect if server error

DEV GUIDELINES:
    - By default the API will use the file with the same name in the last 
      module that was completed. Otherwise, you can specify the module to use file from
    - Suggestion methods shall be prefixed by infer (ex: infer_load_params, infer_mvs)
    - Suggestion methods shall can be plugged as input as params variable of transformation modules
    - Single file modules shall take as input: (pandas_dataframe, params)
    - Single file modules suggestion modules shall ouput (params, log)
    - Single file modules replacement modules shall ouput (pandas_dataframe, log)
    
    - Multiple file modules shall take as input: (pd_dataframe_1, pd_dataframe_2, params)
    - Multiple file modules suggestion modules shall ouput params, log
    - Multiple file modules merge module shall ouput ???
    
    - Files generated by modules should be in module directory and have names determined at the project level (not API, nor module)
    
    - Do NOT return files, instead write files which users can fetch file through the API
    - If bad params are passed to modules, exceptions are raised, it is the 
        APIs role to transform these exceptions in messages
    - Functions to check parameters should be named _check_{variable_or_function} (ex: _check_file_role)
    - All securing will be done in the API part
    - Always return {"error": ..., "project_id": ..., "response": ...} ???

    - All methods to load specific configs should raise an error if the config is not coherent
    - For each module, store user input
    
    - Load all configurations to project variables
    
    - Use _init_project when project_type is a variable in path
    
    - Always include project_type as variable or hardcode
    - Put module name before project_type if it exists for all project_type
    - Put module name after project_type if it exists only for this project_type (only with linker)
    - Put in API code modules that are of use only for the API

NOTES:
    - Pay for persistant storage?

# Transform 
curl -i http://127.0.0.1:5000/transform/ -X POST -F "source=@data/tmp/test_merge.csv" -F "ref=@data/tmp/test_merge_small.csv" -F "request_json=@sample_request.json;type=application/json"

# Upload data
curl -i http://127.0.0.1:5000/download/ -X POST -F "request_json=@sample_download_request.json;type=application/json"

# Download data
curl -i http://127.0.0.1:5000/download/ -X POST -F "request_json=@sample_download_request.json;type=application/json"

# Download metadata
curl -i http://127.0.0.1:5000/metadata/ -X POST -F "request_json=@sample_download_request.json;type=application/json"

USES: /python-memcached

"""

import os

# Change current path to path of api.py
curdir = os.path.dirname(os.path.realpath(__file__))
os.chdir(curdir)

import flask
from flask import Flask, jsonify, render_template, request, send_file, url_for
from flask_session import Session
from flask_socketio import disconnect, emit, SocketIO
from flask_cors import CORS, cross_origin
from werkzeug.utils import secure_filename

from admin import Admin

from normalizer import UserNormalizer
from linker import UserLinker

#==============================================================================
# INITIATE APPLICATION
#==============================================================================

# Initiate application
app = Flask(__name__)
cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
#app.config['SERVER_NAME'] = '127.0.0.1:5000'
app.config['SESSION_TYPE'] = "memcached"# 'memcached'

Session(app)

app.debug = True
app.config['SECRET_KEY'] = open('secret_key.txt').read()
app.config['MAX_CONTENT_LENGTH'] = 2 * 1024 * 1024 * 1024 # Check that files are not too big (2GB)
          
socketio = SocketIO(app)       

#==============================================================================
# HELPER FUNCTIONS
#==============================================================================
    
def _check_privilege(privilege):
    if privilege not in ['user', 'admin']:
        raise Exception('privilege can be only user or admin')

def _check_project_type(project_type):
    if project_type not in ['normalize', 'link']:
        raise Exception('project type can be only normalize or link')

def _check_file_role(file_role):
    if file_role not in ['ref', 'source']:
        raise Exception('File type should be ref or source')

def _check_request():
    '''Check that input request is valid'''
    pass


#def _parse_data_params(proj, data_params, file_role=None):
#    '''
#    Returns identifiers for data based on project history. Uses `get_last_written`
#    to retrieve the last file written given the constraints in data_params
#    INPUT:
#        - proj: a user project
#        - data_params: {'file_role': ..., 'module_name': ..., 'file_name': ...}
#        - file_role: Constrain the file role (for use in linking...)
#    '''
#    if data_params is None:
#        module_name = None
#        file_name = None
#    else:
#        file_role = data_params.setdefault('file_role', file_role)
#        # Skip processing for internal referentials
#        if data_params.setdefault('internal', False):
#            raise Exception('Internal data NOT YET IMPLEMENTED')
#        
#        # Load data from last run (or from user specified)
#        file_name = data_params.setdefault('file_name', None)
#        module_name = data_params.setdefault('module_name', None)
#        
#    if any(x is None for x in [file_role, module_name, file_name]):
#        (file_role, module_name, file_name) = proj.get_last_written(\
#                                        file_role, module_name, file_name)
#    return (file_role, module_name, file_name)

#def _load_from_params(proj, data_params=None):
#    '''Load data to project using the parameters received in request.
#    Implicit load is systematic. TODO: Define implicit load
#    '''
#    (file_role, module_name, file_name) = _parse_data_params(proj, data_params)
#    proj.load_data(module_name, file_name)

def _parse_normalize_request():
    '''
    Separates data information from parameters and assures that values in data
    parameters are safe
    '''
    # Parse json request
    data_params = None
    module_params = None
    if request.json:
        req = request.json
        assert isinstance(req, dict)
    
        if 'data' in req:
            data_params = req['data']
            
            # Make paths secure
            for key, value in data_params.items():
                data_params[key] = secure_filename(value)
            
        if 'params' in req:
            module_params = req['params']
    
    return data_params, module_params
    
def _parse_linking_request():
    data_params = None
    module_params = None
    if request.json:
        params = request.json
        assert isinstance(params, dict)
    
        if 'data' in params:
            data_params = params['data']
            for file_role in ['ref', 'source']:
                # Make paths secure
                for key, value in data_params[file_role].items():
                    data_params[file_role][key] = secure_filename(value)
                
        if 'params' in params:
            module_params = params['params']
    
    return data_params, module_params    


def _init_project(project_type, 
                 project_id=None, 
                 create_new=False, 
                 display_name=None, 
                 description=None):
    '''
    Runs the appropriate constructor for Linker or Normalizer projects
    
    DEV NOTE: Use this in api calls that have project_type as a variable
    '''
    _check_project_type(project_type)
    
    if project_type == 'link':
        proj = UserLinker(project_id=project_id, 
                          create_new=create_new, 
                          display_name=display_name, 
                          description=description)
    else:
        proj = UserNormalizer(project_id=project_id, 
                              create_new=create_new, 
                              display_name=display_name, 
                              description=description)
    return proj
            

#==============================================================================
# WEB
#==============================================================================

@app.route('/')
@app.route('/web/', methods=['GET'])
@cross_origin()
def web_index():
    #  /!\ Partial URL. Full URL will depend on user form
    next_url_link = url_for('web_select_link_project') 
    next_url_normalize = url_for('web_normalize_select_file')
    
    return render_template('index.html',
                       next_url_link=next_url_link, 
                       next_url_normalize=next_url_normalize)

@app.route('/web/link/select_project/', methods=['GET'])
@cross_origin()
def web_select_link_project():
    next_url_partial = url_for('web_link_select_files', project_id='')
    new_link_project_api_url = url_for('new_project', project_type='link')
    delete_project_api_url_partial=url_for('delete_project', project_type='link', project_id='')
    exists_url_partial=url_for('project_exists', project_type='link', project_id='')
    
    admin = Admin()
    list_of_projects = admin.list_project_ids('link')
    
    return render_template('select_link_project.html',
                           list_of_projects = list_of_projects,
                           
                           next_url_partial=next_url_partial,
                           new_link_project_api_url=new_link_project_api_url, 
                           delete_project_api_url_partial=delete_project_api_url_partial,
                           exists_url_partial=exists_url_partial)
    
    
@app.route('/web/normalize/select_file/', methods=['GET']) # (Actually select_project)
@cross_origin()
def web_normalize_select_file():
    MAX_FILE_SIZE = 1048576
    
    next_url_partial = '/web/missing_values/normalize/' #url_for('web_mvs_normalize', file_name='') # Missing project_id and file_name    
    
    new_normalize_project_api_url = url_for('new_project', project_type='normalize')
    delete_normalize_project_api_url_partial=url_for('delete_project', 
                                                     project_type='normalize', 
                                                     project_id='')
    
    admin = Admin()
    all_user_projects = admin.list_projects('normalize') 
    return render_template('select_file_normalize.html', 
                           all_user_projects=all_user_projects,
                           new_normalize_project_api_url=new_normalize_project_api_url,
                           delete_normalize_project_api_url_partial = delete_normalize_project_api_url_partial,
                           
                           upload_api_url_partial=url_for('upload', project_id=''),
                           next_url_partial=next_url_partial,
                           MAX_FILE_SIZE=MAX_FILE_SIZE)    
    

# TODO: look into use of sessions for url generation

@app.route('/web/link/select_files/<project_id>', methods=['GET']) # (Actually select_projects)
@cross_origin()
def web_link_select_files(project_id):
    '''View to create or join 1 or 2 normalization projects (1 for norm, 2 for link)'''
    MAX_FILE_SIZE = 1048576
    
    next_url = url_for('web_mvs_link', project_id=project_id, file_role='source')
    
    new_normalize_project_api_url = url_for('new_project', project_type='normalize')
    delete_normalize_project_api_url_partial=url_for('delete_project', 
                                                     project_type='normalize', 
                                                     project_id='')
    
    # Generate Next URLs
    
    # Do what you fina do
#    proj = _init_project(project_id=project_id)
#    all_csvs = proj._list_files(extensions=['.csv'])
#    
    admin = Admin()
    all_user_projects = admin.list_projects('normalize') # TODO: take care of this
    all_internal_projects = admin.list_projects('normalize') 

#    admin = admin.Admin()
#    list_of_projects = user.list_projects()

    # TODO: If you have link, also add files to current     
    return render_template('select_files_linker.html', 
                           project_id=project_id,
                           all_user_projects=all_user_projects, 
                           all_internal_projects=all_internal_projects,
                           new_normalize_project_api_url=new_normalize_project_api_url,
                           delete_normalize_project_api_url_partial = delete_normalize_project_api_url_partial,
                           
                           upload_api_url_partial=url_for('upload', project_id=''),
                           select_file_api_url=url_for('select_file', project_id=project_id),
                           next_url=next_url,
                           MAX_FILE_SIZE=MAX_FILE_SIZE)




#def _next_url(project_type=None, project_id=None, var=None):
#    # Order if project_type is normalise
#    NORM_ORDER = {'web_index': url_for('web_select_files'),
#                  'web_select_files': url_for('web_mvs'),
#                  'web_mvs': url_for('web_index')
#                  }
#    
#    # Order if project type is link
#    LINK_ORDER = {
#                  'web_index': url_for('web_select_files'),
#                  'web_select_files': url_for('web_mvs'), 
#                  'web_mvs': url_for('web_match_columns'),
#                  'web_match_columns': url_for('web_dedupe'),
#                  'web_dedupe': url_for('web_select_return'),
#                  'web_select_return': url_for('web_download'),
#                  'web_download': url_for('web_index')
#                  }

@app.route('/web/missing_values/normalize/<project_id>/', methods=['GET'])
@app.route('/web/missing_values/normalize/<project_id>/<file_name>/', methods=['GET'])
@cross_origin()
def web_mvs_normalize(project_id, file_name=None):
    # TODO: remove hack with file_name=None
    if file_name is None:
        proj = UserNormalizer(project_id)
        (_, file_name) = proj.get_last_written()
    
    next_url = url_for('web_download_normalize', 
                       project_id=project_id, 
                       file_name=file_name)   
    return _web_mvs_normalize(project_id, file_name, next_url)
    
@app.route('/web/missing_values/link/<project_id>/<file_role>/', methods=['GET'])
@cross_origin()   
def web_mvs_link(project_id, file_role):
    _check_file_role(file_role)
    
    proj = UserLinker(project_id)
    normalize_project_id = proj.metadata['current'][file_role]['project_id']
    normalize_file_name = proj.metadata['current'][file_role]['file_name']
    
    if file_role == 'source':
        next_url = url_for('web_mvs_link', project_id=project_id, file_role='ref')
    else:
        next_url = url_for('web_match_columns', project_id=project_id)
    return _web_mvs_normalize(normalize_project_id, normalize_file_name, next_url)


def _web_mvs_normalize(project_id, file_name, next_url):
    NUM_ROWS_TO_DISPLAY = 30
    NUM_PER_MISSING_VAL_TO_DISPLAY = 4          
    # TODO: add click directly on cells with missing values
    
    # proj_norm = init_normalize_project(project_type=, project_id=project_id)
    
    # Act on last file 
    proj = UserNormalizer(project_id)
    (module_name, file_name) = proj.get_last_written(None, file_name, before_module='replace_mvs') 
    
    # Read config or perform inference for project
    proj.load_data(module_name, file_name)
    mvs_config = proj.read_config_data('replace_mvs', 'config.json')
    if not mvs_config:
        # Infer missing values + save
        mvs_config = proj.infer('infer_mvs', params=None)
    
    # Generate sample to display
    sample_params = {
                    'num_rows_to_display': NUM_ROWS_TO_DISPLAY,
                    'num_per_missing_val_to_display': NUM_PER_MISSING_VAL_TO_DISPLAY,
                    'drop_duplicates': True
                     }
    sample = proj.get_sample('sample_mvs', mvs_config, sample_params)
    
    # Format infered_mvs for display in web app
    formated_infered_mvs = dict()
    formated_infered_mvs['columns'] = {col:[mv['val'] for mv in mvs] \
                    for col, mvs in mvs_config['mvs_dict']['columns'].items()}
    formated_infered_mvs['all'] = [mv['val'] for mv in mvs_config['mvs_dict']['all']]
    
    data_params = {'module_name': module_name, 'file_name': file_name}

    return render_template('missing_values.html',
                           project_id=project_id, 
                           formated_infered_mvs=formated_infered_mvs,
                           index=list(sample[0].keys()),
                           sample=sample,
                           
                           data_params=data_params,
                           add_config_api_url=url_for('upload_config', 
                                                      project_type='normalize',
                                                      project_id=project_id),
                           recode_missing_values_api_url=url_for('replace_mvs', 
                                                      project_id=project_id),
                           next_url=next_url)


@app.route('/web/link/match_columns/<project_id>/', methods=['GET'])
@cross_origin()
def web_match_columns(project_id):
    ROWS_TO_DISPLAY = range(3)
    
    proj = UserLinker(project_id)
    
    # 
    sample_params = {'sample_ilocs':ROWS_TO_DISPLAY}
    
    # Load source and regsample
    samples = dict()
    for file_role in ['ref', 'source']:
        proj.load_project_to_merge(file_role)
        (_, file_name) = proj.__dict__[file_role].get_last_written(module_name=None, 
                                                      file_name=None, 
                                                      before_module='dedupe_linker')
        proj.__dict__[file_role].load_data('INIT', file_name, nrows=max(ROWS_TO_DISPLAY)+1)
        samples[file_role] = proj.__dict__[file_role].get_sample(None, None, sample_params)
        proj.__dict__[file_role].clear_memory()

    
    # Check valid onfirm valid columns for 
    def config_is_coherent(config, source_sample, ref_sample):
        do_break = False
        for pair in config:
            for col in pair['source']:
                if col not in source_sample[0]:
                    do_break = True
                    break
            for col in pair['ref']:
                if col not in ref_sample[0]:
                    do_break = True
                    break     
            if do_break:
                config = []
                break
        
        return config != []

    # Load previous config
    config = proj.read_col_matches()
    config = config * config_is_coherent(config, samples['source'], samples['ref'])                    
    
    return render_template('match_columns.html',
                           config=config,
                           
                           source_index=list(samples['source'][0].keys()),
                           ref_index=list(samples['ref'][0].keys()),
                           
                           source_sample=samples['source'],
                           ref_sample=samples['ref'],
                                                      
                           add_column_matches_api_url=url_for('add_column_matches', project_id=project_id),
                           next_url=url_for('web_dedupe', project_id=project_id))

    
@socketio.on('answer', namespace='/')
def web_get_answer(user_input):
    # TODO: avoid multiple click (front)
    # TODO: add safeguards  if not enough train (front)
    message = ''
    #message = 'Expect to have about 50% of good proposals in this phase. The more you label, the better...'
    if 'labeller' not in dir(flask._app_ctx_stack):
        emit('redirect', {'url': url_for('web_download', project_id=flask._app_ctx_stack.project_id)})
        disconnect
    else:
        if flask._app_ctx_stack.labeller.answer_is_valid(user_input):
            flask._app_ctx_stack.labeller.parse_valid_answer(user_input)
            if flask._app_ctx_stack.labeller.finished:
                print('Writing train')
                flask._app_ctx_stack.labeller.write_training(flask._app_ctx_stack.paths['train'])
                print('Wrote train')
    
                # TODO: Do dedupe
                next_url = url_for('web_select_return', project_type='link', project_id=flask._app_ctx_stack.project_id)
                emit('redirect', {'url': next_url})
            else:
                flask._app_ctx_stack.labeller.new_label()
        else:
            message = 'Sent an invalid answer'
        emit('message', flask._app_ctx_stack.labeller.to_emit(message=message))
    

@socketio.on('skip', namespace='/terminate')
def web_terminate_labeller_load():
    pass


@socketio.on('load_labeller', namespace='/')
def load_labeller():
    '''Loads labeller. Necessary to have a separate call to preload page'''    
    # assert flask.session.project_id == project_id   
    def load_dis_labeller():
        # TODO: put variables in memory
        project_id = flask._app_ctx_stack.project_id
        proj = UserLinker(project_id=project_id)
        flask._app_ctx_stack.proj = proj

        paths = proj._gen_paths_dedupe()  
        flask._app_ctx_stack.paths = paths       
        
        flask._app_ctx_stack.labeller = proj._gen_dedupe_labeller()
        flask._app_ctx_stack.labeller.new_label()
        
        emit('message', flask._app_ctx_stack.labeller.to_emit(message=''))
    
    load_dis_labeller()
    # socketio.start_background_task(load_dis_labeller)


@app.route('/web/link/dedupe_linker/<project_id>/', methods=['GET'])
@cross_origin()    
def web_dedupe(project_id):
    '''Labelling / training and matching using dedupe'''
    
    # Set project ID in session
    flask.session.project_id = project_id
    flask._app_ctx_stack.project_id = project_id
    
    proj = UserLinker(project_id)
    
    dummy_labeller = proj._gen_dedupe_dummy_labeller()
    
    # next_url = url_for('web_dedupe', project_id='{{ project_id }}')
    return render_template('dedupe_training.html', 
                           **dummy_labeller.to_emit(''))
    #return render_template('dedupe_training.html', **DUMMY_EMIT)


@app.route('/web/select_return/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def web_select_return(project_type, project_id):
    '''
    Configurate file to return for ref and source in same page as well as columns
    that are supposed to match to test results (ex: Siren, SIRENE)
    '''
    # TODO: default to matching columns
    _check_project_type(project_type)
    if project_type == 'normalize':
        raise Exception('Normalize project_type is not supported for select_return')
    
    
    ROWS_TO_DISPLAY = range(3)
    
    proj = UserLinker(project_id)
    
    samples = dict()
    selected_columns_to_return = dict()

    for file_role in ['source', 'ref']:
        # Load sample
        data = proj.metadata['current'][file_role]
        
        proj.load_project_to_merge(file_role)
        (module_name, file_name) = proj.__dict__[file_role].get_last_written(None, 
                                                    data['file_name'], 
                                                    before_module='dedupe_linker')
        proj.__dict__[file_role].load_data(module_name, file_name, nrows=max(ROWS_TO_DISPLAY)+1)
        samples[file_role] = proj.__dict__[file_role].get_sample(None, None, {'sample_ilocs':ROWS_TO_DISPLAY})
        
        selected_columns_to_return[file_role] = proj.read_cols_to_return(file_role) 
    
    column_matches = proj.read_col_certain_matches()
    
    indexes = dict()
    indexes['source'] = list(samples['source'][0].keys())
    indexes['ref'] = list(samples['ref'][0].keys())
    
    select_return_api_urls = dict()
    select_return_api_urls['source'] = url_for('add_columns_to_return', 
                                          project_id=project_id, file_role='source')
    select_return_api_urls['ref'] = url_for('add_columns_to_return', 
                                          project_id=project_id, file_role='ref')
    
    next_url = url_for('web_download', project_type=project_type, project_id=project_id)
    return render_template('select_return.html', 
                           indexes=indexes,                           
                           samples=samples,
                           selected_columns_to_return=selected_columns_to_return,    
                           column_matches=column_matches,
                                                              
                           add_column_certain_matches_api_url=url_for(\
                                        'add_column_certain_matches', project_id=project_id),                          

                           select_return_api_urls=select_return_api_urls,
                           next_url=next_url)
    
@app.route('/web/download/normalize/<project_id>/<file_name>', methods=['GET'])
def web_download_normalize(project_id, file_name):
    return '<a href="{0}">Home</a>'.format(url_for('web_index'))

@app.route('/web/download/<project_type>/<project_id>/', methods=['GET'])
def web_download(project_type, project_id):    
    
    if project_type == 'normalize':
        raise NotImplementedError
    
    proj = _init_project(project_type, project_id=project_id)
    
    res_file_name = 'm3_result.csv'
    
    file_path = proj.path_to('dedupe_linker', res_file_name)    
    
    if False or (not os.path.isfile(file_path)):        
        paths = proj.gen_paths_dedupe()
        
        col_matches = proj.read_col_matches()
        my_variable_definition = proj._gen_dedupe_variable_definition(col_matches)
        
        module_params = {
                        'variable_definition': my_variable_definition,
                        'selected_columns_from_source': None,
                        'selected_columns_from_ref': None
                        }  
        
        # TODO: This should probably be moved
        print('Performing deduplication')    
        
        # Perform linking
        proj.linker('dedupe_linker', paths, module_params)
    
        print('Writing data')
        # Write transformations and log
        proj.write_data()    
        proj.write_log_buffer(True)
        
        file_path = proj.path_to(proj.mem_data_info['module_name'], 
                                 proj.mem_data_info['file_name'])
        print('Wrote data to: ', file_path)

    # Identify rows to display
    proj.load_data('dedupe_linker', res_file_name)  

    certain_col_matches = proj.read_col_certain_matches()
    use_lower = True
    metrics = proj.infer('results_analysis', {'col_matches': certain_col_matches, 'lower':use_lower})

    # Choose the columns to display # TODO: Absolutely move this
    col_matches = proj.read_col_matches() # TODO: API this
    suffixes = ('_x', '_y')
    cols_to_display_match = []
    
    #    source_cols = list(set(col for match in col_matches for col in match['source']))
    #    ref_cols = list(set([col for match in col_matches for col in match['ref']] \
    #                                + proj.read_cols_to_return('ref')))

    # TODO: fix for multiple selects of same column
    cols_to_display_match = []
    for col in [_col for match in col_matches for _col in match['source'] + match['ref']]: #\
                #+ proj.read_cols_to_return('ref'):
        if col in cols_to_display_match:
            cols_to_display_match.remove(col)
            cols_to_display_match.append(col + suffixes[0])
            cols_to_display_match.append(col + suffixes[1])
        else:
            cols_to_display_match.append(col)
            
    cols_to_display_match.append('__CONFIDENCE')
    print(cols_to_display_match)
        

    # Choose the columns to display for single source and single ref 
    # TODO: Absolutely change this
    cols_to_display_source = []
    for match in col_matches:
        for col in match['source']:
            if col in cols_to_display_source:
                cols_to_display_source.remove(col)
            else:
                cols_to_display_source.append(col)

    cols_to_display_ref = []
    for match in col_matches:
        for col in match['ref']:
            if col in cols_to_display_ref:
                cols_to_display_ref.remove(col)
            else:
                cols_to_display_ref.append(col)


    # Choose rows to display # TODO: Absolutely move this
    NUM_ROWS_TO_DISPLAY = 1000
    rows_to_display = list(proj.mem_data.index[proj.mem_data.__CONFIDENCE.notnull()])
    rows_to_display = rows_to_display[:NUM_ROWS_TO_DISPLAY + 1]
    
    # Generate display sample
    #    match_sample = proj.get_sample('dedupe_linker', res_file_name,
    #                                row_idxs=rows_to_display, columns=cols_to_display_match)
    sample_params = {
                    'sample_ilocs': rows_to_display,
                    'drop_duplicates': True
                     }
    match_sample = proj.get_sample(None, None, sample_params)
    
    if certain_col_matches:
        sel = proj.mem_data.__CONFIDENCE.notnull()
        if use_lower:
            sel = sel & (proj.mem_data[certain_col_matches['source']].str.lower() \
                         != proj.mem_data[certain_col_matches['ref']].str.lower())
        else:
            sel = sel & (proj.mem_data[certain_col_matches['source']] \
                         != proj.mem_data[certain_col_matches['ref']])
        rows_to_display_error = list(proj.mem_data.index[sel])
        rows_to_display_error = rows_to_display_error[:NUM_ROWS_TO_DISPLAY + 1]
        
        sample_params = {
                        'sample_ilocs': rows_to_display_error,
                        'drop_duplicates': True
                         }        
        
        match_error_samples = proj.get_sample(None, None, sample_params)
    else:
        match_error_samples = []
    
    #    source_sample = proj.get_sample('INIT', proj.metadata['current']['source']['file_name'],
    #                                row_idxs=rows_to_display, columns=cols_to_display_source)
    #ref_sample = proj.get_sample('ref', 'INIT', proj.metadata['current']['ref']['file_name'],
    #                            row_idxs=rows_to_display, columns=cols_to_display_ref)

    return render_template('last_page.html', 
                           project_id=project_id,
                           
                           match_index=cols_to_display_match,
                           match_sample=match_sample,
                           match_error_samples=match_error_samples,                           
                           
                           source_index=cols_to_display_source,
                           ref_index=cols_to_display_ref,
                           # ref_sample=ref_sample,  
                           metrics=metrics,
                           download_api_url=url_for('download', 
                                project_type=project_type, project_id=project_id))



#==============================================================================
# API
#==============================================================================

@app.route('/api/new/<project_type>', methods=['POST'])
def new_project(project_type):
    '''
    Create a new project:
        
    GET:
        - project_type: "link" or "normalize"
    
    '''
    _check_project_type(project_type)
    
    # TODO: include internal in form somewhere
    description = request.json.get('description', '')
    display_name = request.json.get('display_name', '')
    internal = request.json.get('internal', False)
    
    if internal and (not description):
        raise Exception('Internal projects should have a description')

    if project_type == 'normalize':
        proj = UserNormalizer(create_new=True, description=description, display_name=display_name)
    else:
        proj = UserLinker(create_new=True, description=description, display_name=display_name)

    return jsonify(error=False, 
                   project_id=proj.project_id)


@app.route('/api/delete/<project_type>/<project_id>', methods=['GET'])
def delete_project(project_type, project_id):
    """
    Delete an existing project (including all configuration, data and metadata)
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    """
    _check_project_type(project_type)
    # TODO: replace by _init_project
    if project_type == 'normalize':
        proj = UserNormalizer(project_id=project_id)
    else:
        proj = UserLinker(project_id=project_id)
    proj.delete_project()
    return jsonify(error=False)
    


@app.route('/api/metadata/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def metadata(project_type, project_id):
    '''
    Fetch metadata for project ID
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    '''
    proj = _init_project(project_type, project_id=project_id)
    resp = jsonify(error=False,
                   metadata=proj.metadata, 
                   project_id=proj.project_id)
    return resp

@app.route('/api/last_written/<project_type>/<project_id>', methods=['POST'])
def get_last_written(project_type, project_id):
    """
    Get coordinates (module_name, file_name) of the last file written for a 
    given project.
    
    wrapper around: AbstractDataProject.get_last_written
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    POST:
        - module_name: if not null, get last file written in chosen module
        - file_name: if not null, get last file written with this given_name
        - before_module: (contains module_name) if not null, get coordinates 
                            for file written before the chosen module (with an 
                            order specified by MODULE_ORDER)
    """
    proj = _init_project(project_type, project_id)
    (module_name, file_name) = proj.get_last_written(request.json.get('module_name'), 
                          request.json.get('file_name'), 
                          request.json.get('before_module'))
    return jsonify(project_type=project_type, 
                   project_id=project_id, 
                   module_name=module_name, 
                   file_name=file_name)
    
    

@app.route('/api/download/<project_type>/<project_id>', methods=['GET', 'POST'])
@cross_origin()
def download(project_type, project_id):
    '''
    Download specific file from project.
    
    GET:
        - project_type: "link" or "normalize"
        - project_type
        
    POST:
        - module_name: Module from which to fetch the file
        - file_name
    
    '''
    project_id = secure_filename(project_id)

    proj = _init_project(project_id=project_id)
    data_params, _ = _parse_normalize_request()
    
    if data_params is None:
        data_params = {}
        
    file_role = data_params.get('file_role')
    module_name = data_params.get('module_name')
    file_name = data_params.get('file_name')

    if file_role is not None:
        file_role = secure_filename(file_role)
    if module_name is not None:
        module_name = secure_filename(module_name)
    if file_name is not None:
        file_name = secure_filename(file_name)

    (file_role, module_name, file_name) = proj.get_last_written(file_role, module_name, file_name)

    if module_name == 'INIT':
        return jsonify(error=True,
               message='No changes were made since upload. Download is not \
                       permitted. Please do not use this service for storage')

    file_path = proj.path_to(file_role, module_name, file_name)
    return send_file(file_path, as_attachment=True, attachment_filename='m3_merged.csv')


@app.route('/api/link/select_file/<project_id>', methods=['POST'])
def select_file(project_id):
    '''
    Choose a file to use as source or referential for merging
    send {file_role: "source", file_name: "XXX", internal: False}
    
    GET:
        - project_id: ID for the "link" project
        
    POST:
        - file_role: "ref" or "source". Role of the normalized file for linking
        - project_id: ID of the "normalize" project to use for linking
    '''
    proj = UserLinker(project_id)
    params = request.json
    proj.add_selected_project(file_role=params['file_role'], 
                           internal=params.get('internal', False), # TODO: remove internal
                           project_id=params['project_id'])
    return jsonify(error=False)


@app.route('/api/exists/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def project_exists(project_type, project_id):
    '''
    Check if project exists
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    '''
    try:
        _init_project(project_type=project_type, project_id=project_id)
        return jsonify(exists=True)
    except Exception as exc: 
        return jsonify(exists=False)
    

@app.route('/api/normalize/upload/<project_id>', methods=['POST'])
@cross_origin()
def upload(project_id):
    '''
    Uploads files to a normalization project. (NB: cannot upload directly to 
    a link type project)
    
    GET:
        - project_id: ID of the normalization project
        
    POST:
        - file: (csv file) A csv to upload to the chosen normalization project
                NB: the "filename" property will be used to name the file
    '''
    # Load project
    proj = UserNormalizer(project_id=project_id) 
        
    # Upload data
    file = request.files['file']
    if file:
        proj.upload_init_data(file.stream, file.filename)
    else:
        raise Exception('Empty file')
    
    return jsonify(error=False,
               metadata=proj.metadata,
               project_id=proj.project_id)


@app.route('/api/upload_config/<project_type>/<project_id>/', methods=['POST'])
@cross_origin()
def upload_config(project_type, project_id):
    # TODO: do not expose ?
    proj = _init_project(project_type=project_type, project_id=project_id)    
    paths = request.json['data']
    params = request.json['params']
    proj.upload_config_data(params, paths['module_name'], paths['file_name'])
    return jsonify(error=False)

# TODO: get_config if module_name is specified specific module, otherwise, entire project
#@app.route('/api/<project_type>/<project_id>/<module_name>/<file_name>/')
#def get_config(project_type, project_id, module_name=None, file_name=None):
#    '''See docs in abstract_project'''
#    proj = _init_project(project_type, project_id)
#    return proj.get_config(module_name, file_name)

@app.route('/api/link/add_column_matches/<project_id>/', methods=['POST'])
@cross_origin()
def add_column_matches(project_id):
    """
    Add pairs of columns to compare for linking.
    
    wrapper around UserLinker.add_col_matches
    
    GET: 
        - project_id: ID for the "link" project
        
    POST:
        - [list object]: column matches (see doc in original function)
    """
    column_matches = request.json
    proj = UserLinker(project_id=project_id)
    proj.add_col_matches(column_matches)
    return jsonify(error=False)
    

@app.route('/api/link/add_column_certain_matches/<project_id>/', methods=['POST'])
@cross_origin()
def add_column_certain_matches(project_id):
    '''
    Specify certain column matches (exact match on a subset of columns equivalent 
    to entity identity). This is used to test performances.
    
    wrapper around UserLinker.add_col_certain_matches
    
    GET:
        - project_id: ID for "link" project
        
    POST:
        - [list object]: (see doc in original function)
    
    '''
    column_matches = request.json
    proj = UserLinker(project_id=project_id)
    proj.add_col_certain_matches(column_matches)
    return jsonify(error=False)

@app.route('/api/link/add_columns_to_return/<project_id>/<file_role>/', methods=['POST'])
@cross_origin()
def add_columns_to_return(project_id, file_role):
    '''
    Specify columns to be included in download version of file. For link project 
    
    # TODO: shouldn't this be for normalize also ?
    
    wrapper around UserLinker.add_cols_to_return
    
    GET:
        project_id: ID for "link" project
        file_role: "ref" or "source"
    '''
    columns_to_return = request.json
    proj = UserLinker(project_id=project_id)
    proj.add_cols_to_return(file_role, columns_to_return)    
    return jsonify(error=False)

#==============================================================================
# MODULES
#==============================================================================

@app.route('/api/normalize/infer_mvs/<project_id>/', methods=['GET', 'POST'])
@cross_origin()
def infer_mvs(project_id):
    '''
    Runs the infer_mvs module
    
    wrapper around UserNormalizer.infer ?
    
    GET:
        project_id: ID for "normalize" project

    POST:
        - data: {
                "module_name": module to fetch from
                "file_name": file to fetch
                }
        - params: parameters for the inference
    
    '''
    proj = UserNormalizer(project_id=project_id)
    data_params, module_params = _parse_normalize_request()    
    
    proj.load_data(data_params['module_name'], data_params['file_name'])    
    
    result = proj.infer('infer_mvs', module_params)
        
    # Write log
    proj.write_log_buffer(False)
    
    return jsonify(error=False,
                   response=result)

    
@app.route('/api/normalize/replace_mvs/<project_id>/', methods=['POST'])
@cross_origin()
def replace_mvs(project_id):
    '''Runs the mvs replacement module'''
    proj = UserNormalizer(project_id=project_id)
    data_params, module_params = _parse_normalize_request()
    
    proj.load_data(data_params['module_name'], data_params['file_name'])
    
    proj.transform('replace_mvs', module_params)
    # Write transformations and log
    proj.write_data()    
    proj.write_log_buffer(True)
    
    return jsonify(error=False)


@app.route('/api/link/dedupe_linker/<project_id>/', methods=['POST'])
@cross_origin()
def linker(project_id):
    '''
    Runs deduper module. Contrary to other modules, linker modules, take
    paths as input (in addition to module parameters)
    
    {
        'data': {'source': {},  
                'ref': {}}
        'params': {'variable_definition': {...},
                   'columns_to_keep': [...]}
    }
    
    '''
    proj = UserLinker(project_id=project_id) # Ref and source are loaded by default
    data_params, module_params = _parse_linking_request()
    
    # Set paths
    (module_name, file_name) = (data_params['source']['module_name'], data_params['source']['file_name'])
    source_path = proj.source.path_to(module_name=module_name, file_name=file_name)

    (module_name, file_name) = (data_params['ref']['module_name'], data_params['ref']['file_name'])
    ref_path = proj.ref.path_to(file_role='ref', module_name=module_name, file_name=file_name)
    
    paths = {'ref': ref_path, 'source': source_path}
    
    # Perform linking
    proj.linker('dedupe_linker', paths, module_params)

    # Write transformations and log
    proj.write_data()    
    proj.write_log_buffer(True)

    return jsonify(error=False)    
    

#==============================================================================
    # Admin
#==============================================================================


@app.route('/api/projects/<project_type>', methods=['GET'])
def list_projects(project_type):
    '''
    TODO: TEMPORARY !! DELETE FOR PROD !!!
    '''
    admin = Admin()
    list_of_projects = admin.list_project_ids(project_type)
    return jsonify(list_of_projects)


#@app.route('/api/list_normalize_projects/', methods=['GET', 'POST'])
#@cross_origin()
#def list_normalize_projects(user_id=None):
#    '''Lists all project id_s'''
#    raise Exception('Wrong implementation')
#    
#    if user_id is not None:
#        raise NotImplementedError
#    
#    admin = Admin()
#    list_of_projects = admin.list_projects()
#    return jsonify(error=False,
#                   response=list_of_projects)

#@app.route('/api/list_user_projects/<internal>/<user_id>/<project_type>/', methods=['GET', 'POST'])
#@cross_origin()
#def list_projects(internal, user_only=True, project_type=None):
#    '''
#    Lists all project id_s
#    
#    INPUT:
#        - internal: True will return only internal (i.e: visible to all)
#                     False will return all projects
#        - user_only:   
#    '''
#    internal = bool(internal)
#    
#    raise NotImplementedError    
#
#    admin = Admin()
#    list_of_projects = admin.list_projects()
#    return jsonify(error=False,
#                   response=list_of_projects)
#
#
#@app.route('/api/list_referentials/', methods=['GET', 'POST'])
#@cross_origin()
#def list_referentials():
#    '''Lists all internal referentials'''
#    admin = Admin()
#    list_of_projects = admin.list_referentials()
#    return jsonify(error=False,
#                   response=list_of_projects)


if __name__ == '__main__':
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)
